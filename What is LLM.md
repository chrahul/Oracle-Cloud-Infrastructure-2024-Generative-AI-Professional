
# Large Language Models (LLMs)

Large Language Models, or LLMs, are sophisticated probabilistic models of text that utilize vast amounts of data to understand and generate human-like language. At their core, LLMs compute probabilities for sequences of
 words based on the patterns and structures they've learned from the data they were trained on.

Imagine you have a sentence like "I went to the zoo to send me a pet. They sent me a--". A language model like LLM would calculate the probability of various words to fill in the blank based on its understanding of language.
It assigns probabilities to each word in its vocabulary, considering the context provided by the preceding words.

What makes LLMs 'large' is the sheer number of parameters they possess, which enable them to capture intricate nuances of language. However, there isn't a strict threshold defining when a model becomes 'large'.
Despite the term 'large', it's crucial to note that LLMs encompass various sizes, from massive models like GPT-3 to smaller ones like BERT.

These models excel not only in predicting words but also in generating coherent and contextually appropriate text. They achieve this through sophisticated architectures that process input text and generate output text
in a manner resembling human language production.


# LLM Architectures

In this section, I'll be focusing on two major architectures for language models: encoders and decoders. These architectures largely correspond to two different tasks or model capabilities that you may have heard of.

The first capability is embedding, and the second is text generation. Both encoders and decoders are built atop a building block called a transformer.

## Encoders

Encoders are designed to encode text, meaning they produce embeddings. An embedding of text converts a sequence of words into a single vector or a sequence of vectors, aiming to capture the semantics or meaning of the text. Encoders typically take input text and convert it into numerical representations, which can then be used for various downstream tasks such as classification, regression, or semantic search.

Encoders can vary in size, with larger models containing more trainable parameters. However, recent research has shown potential for generating fluent text even with smaller encoder models.

## Decoders

Decoders are responsible for generating text based on the encoded representations produced by encoders. They take a sequence of tokens and emit the next token in the sequence based on the probability of the vocabulary, which is computed using the underlying transformer architecture. Decoders produce one token at a time, and multiple tokens can be generated by invoking the decoder iteratively.

Decoders have shown tremendous capability for generating fluent text and performing tasks such as question answering and dialogue participation.

## Encoder-Decoder Models

Encoder-decoder models combine both encoding and decoding capabilities. They are primarily utilized for sequence-to-sequence tasks like translation. In an encoder-decoder model, the input sequence is passed through an encoder to produce embeddings, which are then decoded into an output sequence by the decoder. Encoder-decoder models have self-referential loops to generate tokens one at a time until the entire sequence is generated.

Here's a table summarizing various tasks and the corresponding model architectures historically used to accomplish them:

| Task              | Encoder Model | Decoder Model | Encoder-Decoder Model |
|-------------------|---------------|---------------|-----------------------|
| Text Embedding    | ✓             |               |                       |
| Text Generation   |               | ✓             |                       |
| Translation       |               |               | ✓                     |
| Question Answering|               |               | ✓                     |
| Dialogue Systems  |               |               | ✓                     |

This concludes our discussion on language model architectures. In the next lesson, we'll discuss affecting the model's distribution over tokens with a technique called prompting.
```

This Markdown format allows for clear organization and presentation of the content, making it easy to read and understand.


